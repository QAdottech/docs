---
title: GitHub App for PR Reviews
description: 'Automatic AI-powered pull request testing and reviews'
icon: 'robot'
---

Get autonomous AI-powered test coverage and reviews on every pull request. The QA.tech GitHub App analyzes code changes, creates missing tests, and posts comprehensive reviews based on test results.

<Note>
Looking to manually trigger tests from CI/CD workflows? See [GitHub Actions](/configuration/github-actions) instead.
</Note>

## What It Does

| Feature | Description |
|:--------|:------------|
| ✅ **Intelligent Test Selection** | AI semantically matches PR changes to relevant tests (typically 5-15 tests selected) |
| ✅ **Gap-Only Test Generation** | Creates 1-3 tests only when coverage gaps exist; most PRs create zero new tests |
| ✅ **Persistent Test Suite** | Auto-generated tests become permanent regression tests for future PRs |
| ✅ **Preview Environment Testing** | Tests against PR preview deployments |
| ✅ **Approval/Rejection** | Posts reviews with pass/fail verdicts |
| ❌ **Manual Control** | No control over which tests run (use Actions for that) |
| ❌ **Custom Workflows** | No YAML workflows needed or supported |

<Steps>
  <Step title="Install GitHub App">
    Go to [Settings → Organization → Connections](https://app.qa.tech/dashboard/current-project/settings/organization/connections) and add the GitHub App connection. Follow the OAuth flow to grant access to your repositories.
  </Step>

  <Step title="Select Repository">
    Navigate to [Settings → Integrations](https://app.qa.tech/dashboard/current-project/settings/integrations?focus=github-app) and select the repository you want to enable PR reviews for.
    
    **PR reviews are enabled automatically** once you select a repository. 
  </Step>

  <Step title="Create a Pull Request">
    Once enabled, the agent automatically:
    1. Detects code changes when PRs are opened or updated
    2. Determines which tests are relevant
    3. Creates new tests for untested functionality
    4. Runs all relevant tests against the PR preview
    5. Posts a review with approval or decline based on results
  </Step>
</Steps>

## Understanding GitHub Deployments

GitHub Deployments are GitHub's native way of tracking when code is deployed to an environment. When your CI/CD deploys a PR, it can create a deployment record that includes the environment name and URL.

**Platforms with automatic GitHub deployment integration:**
- **Vercel** - Creates GitHub deployments automatically for every PR
- **Netlify** - Auto-registers deployments when configured
- **Render, Railway, Fly.io** - Most modern platforms support this

**For other platforms:** You'll need to manually create GitHub deployment records in your CI/CD pipeline. Need help setting this up? Contact us at support@qa.tech.

<Note>
Without GitHub deployments, QA.tech won't know which URL to test your PR against. Environment mapping only works when your CI/CD creates these deployment records.
</Note>

## Environment Mapping

Map GitHub deployment environments to QA.tech Applications so tests run against the correct preview URLs.

**When to use:**
- You have multiple Applications (frontend, backend, etc.)
- Your CI/CD creates GitHub deployment environments
- You want tests to run against PR-specific URLs

**How it works:**
1. Your CI/CD deploys a PR and creates a GitHub environment (e.g., "Preview" or "pr-123")
2. QA.tech detects the deployment
3. Tests run using the mapped Application's URL from that environment

**Location:** Settings → Integrations → GitHub App → Map Environments

## How PR Reviews Work

### Agent Workflow

```
PR opened/updated
  ↓
1. Classify Changes
   → User-facing? Continue
   → Docs/infra only? Skip testing, post info comment
  ↓
2. Assess Coverage
   → Find relevant existing tests
   → Identify gaps in coverage
  ↓
3. Create Tests (if needed)
   → Generate tests for untested functionality
   → Configure dependencies (e.g., login tests)
  ↓
4. Run Tests
   → Execute against PR preview environment
   → Wait for completion
  ↓
5. Post Review
   → ✅ Approve if all tests pass
   → ❌ Decline if tests fail
   → ℹ️ Informational if untestable
```

## Understanding Test Selection & Creation

The PR Review agent uses AI to intelligently select and create tests. Here's how it decides what to run and when to create new tests.

### How Test Selection Works

The agent matches PR changes to test goals semantically, not through keyword matching:

**Example: PR changes checkout payment flow**
```
Your project: 645 total tests

Agent analyzes:
├─ "Complete checkout with credit card" → RELEVANT ✓
├─ "Complete checkout with PayPal" → RELEVANT ✓  
├─ "Verify payment confirmation email" → RELEVANT ✓
├─ "User profile photo upload" → NOT RELEVANT ✗
└─ "Search products by category" → NOT RELEVANT ✗

Selected: 12 tests covering payment & checkout flows
```

**Key mechanics:**
- AI semantically matches changed files to test goals
- Runs ALL tests it determines are relevant (no arbitrary limits)
- Only considers tests with `status='enabled'` (skips draft/error tests)
- Better than fixed CI that runs all 645 tests every time

### Typical Test Counts

| PR Type | Existing Tests Selected | New Tests Created | Total |
|:--------|:-----------------------|:------------------|:------|
| Bug fix in login | 3-7 | 0 (already covered) | 3-7 |
| Small feature | 5-10 | 1-2 (fill gaps) | 6-12 |
| Major feature | 15-25 | 3-5 (new functionality) | 18-30 |
| Refactor | 10-20 | 0 (no new behavior) | 10-20 |
| Docs/infra only | 0 | 0 (untestable via UI) | 0 |

**No upper limit** - the agent runs as many tests as needed for confidence, optimized for coverage not speed.

### Creating Tests for Coverage Gaps

Tests are created **only when gaps exist** - not for every PR.

**Example: PR adds Apple Pay to checkout**
```
Agent assesses existing coverage:
├─ "Complete checkout with credit card" ✓ Exists
├─ "Complete checkout with PayPal" ✓ Exists
└─ Apple Pay integration ✗ Gap identified

Decision: Create 1 new test
→ "Complete checkout with Apple Pay"
```

**When tests ARE created:**
- New features with zero coverage
- New user flows not tested before
- New payment methods, auth methods, etc.

**When tests are NOT created (most common):**
- Bug fixes (existing tests already cover the flow)
- Refactors (no new user-facing behavior)
- Code improvements (same functionality)
- Documentation/infrastructure changes

**Reality check - 100 PRs:**
- ~60 PRs: Bug fixes/refactors → 0 new tests
- ~25 PRs: Docs/infra → 0 new tests  
- ~10 PRs: Small features → 1 test each = 10 tests
- ~5 PRs: Major features → 3 tests each = 15 tests
- **Total: ~25 new tests, NOT 300**

### Auto-Generated Test Lifecycle

Tests created during PR review become permanent regression tests:

**The Journey:**
```
PR #42: Add Apple Pay
├─ Agent identifies gap
├─ Creates "Complete checkout with Apple Pay"  
├─ Test runs on PR #42 ✅
├─ Test persists in your suite (labeled 'ephemeral')
└─ Available for future PRs

PR #58: Refactor checkout UI
├─ Agent finds "Complete checkout with Apple Pay" exists
├─ Runs existing test (no new test created) ✅
└─ Your suite protects against Apple Pay regressions
```

**Key points:**
- Tests persist permanently (not temporary/deleted after PR)
- Labeled 'ephemeral' to indicate AI-generated pending review
- Automatically selected for future relevant PRs
- Suite grows intelligently with zero maintenance

**Managing auto-generated tests:**
- Review tests in UI (Settings → Test Cases, filter by 'ephemeral')
- Enable/refine tests removes 'ephemeral' label
- Disable or delete tests you don't want
- Tests work fine as-is, review is optional

### Preventing Test Overlap

The agent actively prevents duplicate tests through semantic understanding:

**How it works:**
1. Agent calls `getTestCases` before creating ANY new tests
2. Reviews ALL existing test names, goals, and coverage areas
3. Explicitly instructed: "avoid semantic duplication with existing tests"
4. Understands functional overlap (not just name matching)

**Example - Avoiding Duplicates:**

| Existing Test | PR Change | Agent Decision |
|:-------------|:----------|:---------------|
| "User login with credentials" | Add OAuth | Create "User login with Google OAuth" (different method) |
| "Complete checkout flow" | Fix checkout bug | Skip creation (already covered) |
| "Test payment processing" | Add refunds | Create "Process refund" (new functionality) |

**Why some overlap may still occur:**
- AI isn't perfect (may miss subtle semantic overlap)
- Intentional coverage from different angles is valuable
- Better to have slight redundancy than missed coverage

**Your control:**
- Review auto-generated tests (labeled 'ephemeral')
- Disable redundant tests in UI
- Delete duplicates if created
- System never auto-deletes tests

### Self-Limiting Growth

As your test suite grows, the system creates fewer tests automatically:

```
Early PRs:
├─ Small test suite = Many gaps
└─ More test generation

Later PRs:
├─ Large test suite = Better coverage
└─ Mostly reuses existing tests

Result: Test creation naturally slows as coverage improves
```

### Review Format

The agent posts a review with:

- **Verdict:** ✅ Tests passing / ❌ Tests failing / ℹ️ Unable to verify
- **What was tested:** Description of coverage in prose
- **Results summary:** Patterns and themes from test outcomes
- **Test details:** Automatic table with individual test results

**What reviews DON'T include:**
- Code quality opinions
- Implementation suggestions
- References to other bot comments

## Best Practices

**Deploy with Vercel or Netlify**

These platforms automatically create GitHub deployments, making PR testing seamless. No additional configuration needed - just connect your repository and every PR gets tested automatically.

**Your test suite grows automatically**

Tests created during PR reviews persist permanently and become regression tests automatically. No manual promotion needed - the agent reuses these tests on future PRs when relevant. Review auto-generated tests (labeled 'ephemeral') in Settings → Test Cases to refine or disable them.

**Let QA.tech test PRs first**

Configure PR reviews to run before requesting human code review. This catches functional issues early, so your colleagues can focus on code quality instead of manually testing features.

**Customize reviews with context**

Add domain-specific instructions to guide the AI agent's testing approach. Go to Settings → Integrations → GitHub App and add review context.

Example for payment flows:
```
When testing payment flows, always verify:
- Stripe test mode is active
- Receipt emails are sent
- Transaction appears in admin dashboard
```

Use cases:
- Specify areas requiring extra scrutiny
- Provide domain-specific testing guidelines
- Add authentication/setup requirements


## Frequently Asked Questions

### Will auto-generated tests pollute my test suite?

No. The agent only creates tests for coverage gaps - most PRs (bug fixes, refactors) create zero new tests. Even major feature PRs typically add 3-5 focused tests, not hundreds. The system is self-limiting: better coverage means fewer gaps, which means less test generation over time.

### Can I control which tests run?

No - the GitHub App is fully autonomous for speed and simplicity. For manual test selection and control, use [GitHub Actions](/configuration/github-actions) instead. You can use both: GitHub App for automatic PR reviews + Actions for manual deep testing.

### How do I review or delete auto-generated tests?

Go to Settings → Test Cases and filter by 'ephemeral' label. You can enable (removes label), edit, disable, or delete any test. Tests work fine as-is - review is optional.

### What if the agent creates duplicate tests?

The agent actively prevents duplicates by reading all existing tests first and using semantic deduplication. If a duplicate slips through (AI isn't perfect), simply disable or delete it in the UI. Better to occasionally have slight redundancy than miss coverage gaps.

## Troubleshooting

### PR reviews aren't posting

**Check:**
- GitHub App installed and granted repository access
- Repository integration configured in QA.tech
- PR reviews are enabled for this repository 
- PR has user-facing changes (not docs/infra only)

### Tests running against wrong URL

**Solutions:**
- Map your GitHub environments to Applications (Settings → Integrations → GitHub App → Map Environments)
- Verify your CI/CD creates GitHub deployment environments
- Check that environment names match between GitHub and QA.tech

### Agent created irrelevant tests

**Solutions:**
- Add review context with specific testing guidelines
- Update existing tests to cover the functionality better
- The agent learns from your existing test patterns

### Review says "Unable to verify through end-to-end testing"

**This is expected for:**
- Documentation-only changes (.md, .txt, README)
- Infrastructure changes (CI/CD, Docker, deployment scripts)
- Build configuration (package.json without functional changes)

**The agent can only test changes accessible through the UI.**

## Limitations

- **Fully autonomous:** Cannot manually select specific tests (use GitHub Actions for control)
- **Requires preview deployments:** Cannot test without accessible PR environment
- **UI-testable changes only:** Backend-only microservices without UI access can't be tested
- **No workflow customization:** Unlike GitHub Actions, there's no YAML configuration

## Need Manual Control?

If you need to:
- Choose specific test plans to run
- Trigger tests from custom CI/CD steps
- Control test execution timing
- Run deep exploratory testing on-demand

See [GitHub Actions](/configuration/github-actions) for manual CI/CD integration and on-demand exploratory testing via `@qatech` mentions. You can use both approaches for comprehensive coverage.

