---
title: Running Tests
description: "Execute AI-generated tests and interpret results in QA.tech's cloud-based platform"
icon: play
---

# Running AI-Powered Tests

QA.tech's AI agents can execute tests automatically based on your natural language descriptions. Learn how to run tests, interpret AI-generated results, and manage test execution across different environments.

## Understanding AI Test Execution

### How AI Agents Run Tests

When you run a test in QA.tech, the AI agent:

1. **Analyzes your test description** - Understands what you want to test
2. **Navigates your application** - Uses visual understanding to interact with UI elements
3. **Executes the test flow** - Performs actions step-by-step like a human would
4. **Captures results** - Records screenshots, videos, and detailed logs
5. **Reports findings** - Provides comprehensive results with any issues found

### Test Execution Process

<Steps>
  <Step title="AI Agent Initialization">
    The AI agent starts with your test description and application context
  </Step>
  <Step title="Visual Analysis">
    AI scans the page to understand available elements and interactions
  </Step>
  <Step title="Step-by-Step Execution">
    AI performs each action, waiting for responses and adapting to changes
  </Step>
  <Step title="Result Capture">
    AI records screenshots, videos, and detailed execution logs
  </Step>
  <Step title="Analysis and Reporting">
    AI analyzes results and provides comprehensive test reports
  </Step>
</Steps>

## Application Navigation

### Primary Navigation
The left sidebar provides access to core application sections:
- **Dashboard**: Project overview and recent activity
- **Test Cases**: Individual test case management (with notification badge)
- **Test Plans**: Test case grouping and execution planning
- **Results**: Test execution results and history
- **Issues**: Bug tracking and improvement management
- **Insights**: Analytics and performance metrics
- **Settings**: Configuration management

### AI Chat Section
- **Location**: Below main navigation in left sidebar
- **Function**: "Test creation request" with timestamp
- **Action**: "New" button to start new chat
- **Purpose**: AI-powered test creation and assistance

### Test Runs Counter
- **Display**: "1 of 10" indicating current usage
- **Warning**: Yellow banner "This is just a trial, upgrade for more tests! You have almost reached the limit for the trial."
- **Action**: "Upgrade Now" button
- **Location**: Below AI Chat section in left sidebar

### Project Selector
- **Location**: Bottom of left sidebar
- **Function**: Dropdown to switch between projects
- **Display**: Shows current project (e.g., "test Demo CRM")
- **Action**: Collapse sidebar option available

### Additional Navigation Elements
- **Book a call with us**: Support link in left sidebar
- **Documentation**: Help link in left sidebar
- **Collapse Sidebar**: Option to minimize navigation

### Settings Structure
Settings are organized into logical categories:
1. **Project Settings**: Application-specific configurations (General, Applications (Beta), Configs, Integrations, Network, Inbox, Device Presets, Knowledge)
2. **Organization Settings**: Team and billing management (General, Members, Projects, Subscription, Connections)
3. **Profile Settings**: Personal account settings (My Details, Authentication, Email, Password)

### Quick Actions
Dashboard provides quick access to:
- **Create a Test**: Test case creation for existing functionality
- **Reproduce a Bug**: Test case creation for bug verification
- **Start a Chat**: AI-powered test creation and assistance

## Running Tests from the Web Interface

### Individual Test Execution

<Steps>
  <Step title="Navigate to Test Cases">
    Go to [Test Cases](https://app.qa.tech/dashboard/current-project/scenarios) in your project
  </Step>
  <Step title="Select Test to Run">
    Find the test you want to run and click the **"Run Test"** button
  </Step>
  <Step title="Monitor Execution">
    Watch the AI agent execute your test in real-time
  </Step>
  <Step title="Review Results">
    Examine detailed results, screenshots, and any issues found
  </Step>
</Steps>

### Batch Test Execution

<Steps>
  <Step title="Select Multiple Tests">
    Choose multiple tests from your test cases list
  </Step>
  <Step title="Run Test Plan">
    Click **"Run Tests"** to execute all selected tests
  </Step>
  <Step title="Monitor Progress">
    Track execution progress across all tests
  </Step>
  <Step title="Review Batch Results">
    Analyze results for all tests in the batch
  </Step>
</Steps>

## Understanding Test Results

### Viewing Test Results

**Navigation:** Results ‚Üí Left sidebar navigation

**Result Display Format:**
Test results are presented in a table format with the following columns:
- **ID**: Unique test run identifier (e.g., "hq76")
- **Title**: Test run description (e.g., "Custom")
- **Trigger**: How the test was initiated (e.g., "Manual by testuser@test.com")
- **Result**: Overall test status displayed as "X Passed, Y Failed" with colored dots (e.g., "7 Passed, 1 Failed")
- **Duration**: Total execution time (e.g., "3m 41s")
- **Started**: Test run initiation timestamp (e.g., "6 days ago")

**Detailed Results:**
Clicking on a test run provides detailed results organized by test categories:
- **Dashboard**: Dashboard-related tests (e.g., "Verify that Total Customers widget loads", "Go to Dashboard", "Test the loading of the Recent Customers widget")
- **Companies**: Company management tests (e.g., "Navigate to Companies Page Test Case")
- **Customers**: Customer management tests (e.g., "Navigate to Customers Page", "Validation of Mandatory Customer Name Field", "Ensure Email Required for Customer Creation", "Add a new customer")

Each category shows individual test status, issues, started time, duration, and last 5 runs history with colored dots indicating pass/fail status.

**Failed Tests Section:**
- **Location**: Top of the detailed results page, below the progress bar
- **Content**: Lists specific failed tests with descriptions
- **Example**: "Test the loading of the Recent Customers widget" with description "The Recent Customers widget shows 'Can't connect to database' instead of customer data."
- **Visual Indicators**: Red dots for failed tests, green dots for passed tests

**Progress Visualization:**
- **Progress Bar**: Visual representation of test results with green segments for passed tests and red segments for failed tests
- **Summary Box**: Shows status (Failed/Passed), manual trigger information, start time, duration, and visual summary of passed/failed counts

**Note:** The system reports test-level results, not individual step-level failures within tests.

### Result Components

**Execution Summary:**
- Overall test status (PASSED/FAILED)
- Number of tests passed vs failed
- Total execution time
- Test run duration
- Screenshots and video evidence

**Visual Evidence:**
- Screenshots of each interaction
- Video recording of full execution
- Highlighted areas of interaction
- Error screenshots if failures occur

**Detailed Logs:**
- AI decision-making process
- Element selection reasoning
- Wait strategies used
- Error handling details

### Common Result Patterns

#### **Successful Test Execution**
```
‚úÖ All tests completed successfully
üì∏ Screenshots captured at key points
üé• Video recording available
‚è±Ô∏è Total execution time: 30-60 seconds
```

#### **Test with Issues**
```
‚ö†Ô∏è Test completed with issues
üì∏ Screenshot captured at failure point
üí° AI suggestions for improvement
```

#### **AI Learning Results**
```
üß† AI learned from this execution
üìà Improved element recognition
üîÑ Better wait strategies identified
üìù Updated knowledge for future tests
```

## CI/CD Integration

### GitHub Actions Integration

Set up automated test execution with GitHub Actions:

```yaml
name: QA.tech AI Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - name: Run AI-Powered Tests
      uses: QAdottech/run-action@v2
      with:
        project_id: ${{ secrets.QATECH_PROJECT_ID }}
        api_token: ${{ secrets.QATECH_API_TOKEN }}
        test_plan_short_id: 'jgbinp'
        blocking: true
```

### GitLab CI Integration

Configure GitLab CI for automated testing:

```yaml
stages:
  - test

qa-tech-tests:
  stage: test
  image: node:18
  before_script:
    - echo "QA.tech tests will be executed via GitHub Actions"
  script:
    - echo "Using QA.tech cloud-based testing platform"
  environment:
    name: staging
  variables:
    QA_TECH_ENVIRONMENT: staging
    QA_TECH_TIMEOUT: 60000
    QA_TECH_RETRY_COUNT: 3
```

## Scheduled Test Execution

### Setting Up Automated Schedules

<Steps>
  <Step title="Navigate to Results">
    Go to [Results](https://app.qa.tech/dashboard/current-project/results)
  </Step>
  <Step title="Configure Schedule">
    Click the **"Schedule"** button to set up automated runs
  </Step>
  <Step title="Define Schedule">
    Choose frequency: daily, weekly, or custom cron schedule
  </Step>
  <Step title="Select Tests">
    Choose which tests to include in the scheduled run
  </Step>
  <Step title="Set Notifications">
    Configure email or webhook notifications for results
  </Step>
</Steps>

### Schedule Examples

**Daily Regression Testing:**
```
Schedule: Daily at 2:00 AM
Tests: All critical user flows
Environment: Staging
Notifications: Email on failures
```

**Weekly Full Test Suite:**
```
Schedule: Every Sunday at 1:00 AM
Tests: Complete test suite
Environment: Production
Notifications: Slack webhook
```

## Environment-Specific Execution

### Running Tests Across Environments

<Steps>
  <Step title="Select Environment">
    Choose the environment for your test execution
  </Step>
  <Step title="Configure Environment Settings">
    Set up environment-specific configurations
  </Step>
  <Step title="Execute Tests">
    Run tests with environment-specific settings
  </Step>
  <Step title="Compare Results">
    Analyze results across different environments
  </Step>
</Steps>

### Environment Configuration

```json
{
  "environments": {
    "development": {
      "url": "https://dev.your-app.com",
      "timeout": 30000,
      "debug_mode": true
    },
    "staging": {
      "url": "https://staging.your-app.com",
      "timeout": 60000,
      "debug_mode": false
    },
    "production": {
      "url": "https://your-app.com",
      "timeout": 90000,
      "debug_mode": false
    }
  }
}
```

## Monitoring Test Execution

### Real-Time Monitoring

<Steps>
  <Step title="Track Execution Progress">
    Monitor test execution in real-time
  </Step>
  <Step title="Review AI Decisions">
    Understand how AI agents make decisions
  </Step>
  <Step title="Analyze Performance">
    Track execution times and success rates
  </Step>
  <Step title="Identify Patterns">
    Spot trends in test results and AI behavior
  </Step>
</Steps>

### Performance Metrics

**Execution Speed:**
- Average test duration: 30-60 seconds
- Parallel execution capabilities
- Environment-specific performance

**Success Rates:**
- Track pass/fail ratios
- Monitor flaky test patterns
- Analyze failure root causes

**AI Learning:**
- Element recognition improvements
- Wait strategy optimizations
- Context understanding enhancements

## Troubleshooting Test Execution

### Common Execution Issues

**Problem:** Tests taking too long to execute

**Solutions:**
- Optimize test descriptions for clarity
- Check application performance
- Review AI agent configuration
- Consider parallel execution

**Problem:** AI agent can't find elements

**Solutions:**
- Improve element descriptions in test prompts
- Check application accessibility
- Verify element visibility
- Use AI chat for debugging

**Problem:** Inconsistent test results

**Solutions:**
- Review application stability
- Check for dynamic content
- Optimize wait strategies
- Improve test descriptions

## Best Practices

### Test Execution Optimization

<Card title="‚úÖ Do" icon="check">
  - Write clear, specific test descriptions
  - Use environment-specific configurations
  - Monitor execution performance
  - Review and learn from AI decisions
</Card>

<Card title="‚ùå Don't" icon="x">
  - Write overly complex test descriptions
  - Ignore execution performance metrics
  - Skip result analysis and learning
  - Run tests without proper environment setup
</Card>

### Continuous Improvement

**Regular Review:**
- Analyze test execution patterns
- Learn from AI agent decisions
- Optimize test descriptions
- Update environment configurations

**Performance Monitoring:**
- Track execution times
- Monitor success rates
- Identify bottlenecks
- Optimize for efficiency

## Next Steps

- **[Test Results Analysis](/best-practices/test-results)** - Learn how to interpret AI-generated results
- **[Troubleshooting](/best-practices/troubleshooting)** - Resolve common execution issues
- **[Authentication](/configuration/authentication)** - Multi-factor authentication and account security
- **[Creating Tests](/best-practices/creating-tests)** - Write effective test descriptions
